# ğŸ§  Multi-Modal RAG System

A versatile, lightweight Retrieval-Augmented Generation (RAG) system designed to handle all major content formats â€” videos, audio, images, text, PDFs, DOCX files, and URLs â€” making your entire digital library searchable and chat-friendly. Powered by OpenAIâ€™s GPT models, this tool allows users to interact with their data using natural language queries.

Whether itâ€™s a PDF report, a recorded interview, a presentation screenshot, or even a YouTube video â€” this system extracts relevant content, indexes it for fast retrieval, and returns intelligent, grounded responses with source references.
ğŸŒŸ What Makes This Special?

## âœ… Universal Input Support: Works with almost any type of content â€” not just documents, but also images, audio, videos, web links, and more.

ğŸ™ï¸ Audio/Video Understanding: Uses Whisper to extract insights from spoken content in interviews, lectures, or podcasts, and makes them queryable just like documents.

ğŸ–¼ï¸ Image Intelligence: OCR-based extraction lets you search and question scanned documents, charts, presentations, and screenshots.

ğŸ“„ URL Parsing (Coming Soon): Automatically extracts and embeds content from web pages using their URLs.

âš¡ Fast Retrieval + GPT Reasoning: Combines FAISS vector search with OpenAI GPT to return fast, relevant, and explainable answers â€” grounded in your own data.

ğŸ’» Minimal Setup, Modular Design: Built to be easy to extend or plug into larger systems â€” great for hobbyists and production users alike.

ğŸ§  LLM-Powered QA Interface: A polished Streamlit app allows you to upload files, ask questions, and browse answers â€” all in one place.

## ğŸš€ Features

- ğŸ“„ Extracts content from **PDFs**, **Word documents**, **images**, and **videos**
- ğŸ§  Transcribes audio using **Whisper** for video inputs
- ğŸ§¬ Embeds text using **Sentence-Transformers**
- ğŸ“¦ Stores vectors using **FAISS** for fast retrieval
- ğŸ’¬ Generates answers using **GPT-4**
- ğŸ–¥ï¸ Clean **Streamlit UI** for asking questions and viewing sources

---

## ğŸ› ï¸ Tech Stack

| Task                        | Library                     |
|----------------------------|-----------------------------|
| Text extraction (PDF/DOCX) | PyMuPDF, python-docx        |
| Image OCR                  | pytesseract, Pillow         |
| Video/audio transcription  | OpenCV, Whisper             |
| Embedding                  | sentence-transformers       |
| Vector DB                  | FAISS                       |
| LLM                        | OpenAI GPT (via API)        |
| UI                         | Streamlit                   |

---

## ğŸ“‚ Project Structure

```bash

â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ingestion.py      # Extracts text/audio from files
â”‚   â”œâ”€â”€ embedder.py       # Embeds documents into vectors
â”‚   â”œâ”€â”€ retriever.py      # FAISS-based vector store
â”‚   â”œâ”€â”€ generator.py      # Prompt building + GPT query
â”‚   â””â”€â”€ ui.py             # Streamlit frontend
â”œâ”€â”€ data/                 # Your document uploads
â”œâ”€â”€ vector_db/            # Saved FAISS index and metadata
â”œâ”€â”€ build_index.py        # Script to build FAISS index
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md


âœ… Getting Started

1. Clone the Repository

git clone https://github.com/Abhi21sar/Multi-Modal-RAG-System.git
cd Multi-Modal-RAG-System

2. Install Requirements

Itâ€™s recommended to use a virtual environment.

pip install -r requirements.txt

3. Add Your Documents

Place all your PDF, DOCX, image (.jpg, .png), or video (.mp4, .mov) files inside the data/ folder.

â¸»

ğŸ—ï¸ Build the Vector Index

python build_index.py

This will:
	â€¢	Extract and parse all supported files
	â€¢	Generate embeddings using sentence-transformers
	â€¢	Store them in a FAISS index under vector_db/

â¸»

ğŸ’¬ Launch the Assistant

streamlit run app/ui.py

This opens a web interface where you can:
	â€¢	Ask questions about your uploaded files
	â€¢	See the AI-generated response
	â€¢	Expand to view the supporting document excerpts

â¸»

ğŸ”‘ Setup OpenAI API

In app/generator.py, replace:

client = OpenAI(api_key="Your_OPENAI_API_KEY")

with your actual OpenAI API key.

â¸»

âœï¸ Example Use Cases
	â€¢	ğŸ“ â€œSummarize this 100-page research paper PDF.â€
	â€¢	ğŸ–¼ï¸ â€œWhatâ€™s mentioned on the whiteboard in this image?â€
	â€¢	ğŸ™ï¸ â€œWhat were the key points discussed in this podcast?â€
	â€¢	ğŸï¸ â€œExtract action items from the Zoom meeting recording.â€
	â€¢	ğŸŒ â€œGive a summary of the article at this URL.â€ (coming soon)

â¸»

ğŸ“Œ Roadmap & Future Improvements
	â€¢	ğŸ§  Add image embeddings via OpenAI CLIP
	â€¢	ğŸ”— RAG orchestration with LangChain
	â€¢	ğŸŒ URL crawling and semantic parsing
	â€¢	ğŸ§© Hybrid search: vector + keyword
	â€¢	â˜ï¸ One-click Streamlit Cloud deployment

â¸»

ğŸ¤ Contributing

Pull requests are welcome! If you have ideas to improve multi-modal handling or LLM prompting, feel free to open an issue or PR.

â¸»

ğŸ“„ License

MIT License

â¸»

ğŸ™‹â€â™‚ï¸ Author

Abhishek Gurjar
GitHub: @Abhi21sar
Happy shipping! ğŸš€
